# Example usage of HDFS model saving and loading

from pyspark.sql import SparkSession
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression

from src.train import save_model, load_model_from_hdfs

# Initialize Spark with HDFS support
def create_spark_session_with_hdfs():
    """T·∫°o SparkSession v·ªõi c·∫•u h√¨nh HDFS"""
    spark = SparkSession.builder \
        .appName("MLModelWithHDFS") \
        .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
        .config("spark.sql.warehouse.dir", "hdfs://namenode:9000/user/hive/warehouse") \
        .config("spark.hadoop.dfs.client.use.datanode.hostname", "true") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .getOrCreate()
    
    return spark

# Example: Train and save model to HDFS
def train_and_save_model_example():
    """V√≠ d·ª• hu·∫•n luy·ªán v√† l∆∞u model l√™n HDFS"""
    
    # T·∫°o SparkSession
    spark = create_spark_session_with_hdfs()
    
    # T·∫°o d·ªØ li·ªáu m·∫´u
    data = spark.createDataFrame([
        (1.0, 2.0, 3.0, 10.0),
        (2.0, 3.0, 4.0, 20.0),
        (3.0, 4.0, 5.0, 30.0),
        (4.0, 5.0, 6.0, 40.0),
        (5.0, 6.0, 7.0, 50.0)
    ], ["feature1", "feature2", "feature3", "label"])
    
    # T·∫°o pipeline
    assembler = VectorAssembler(
        inputCols=["feature1", "feature2", "feature3"],
        outputCol="features"
    )
    
    lr = LinearRegression(featuresCol="features", labelCol="label")
    
    pipeline = Pipeline(stages=[assembler, lr])
    
    # Hu·∫•n luy·ªán model
    print("ƒêang hu·∫•n luy·ªán model...")
    model = pipeline.fit(data)
    
    # L∆∞u model l√™n HDFS
    hdfs_model_path = "hdfs://namenode:9000/models/stock_prediction_model"
    print(f"ƒêang l∆∞u model l√™n HDFS: {hdfs_model_path}")
    
    success = save_model(model, hdfs_model_path)
    if success:
        print("Model ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh c√¥ng l√™n HDFS!")
    else:
        print("C√≥ l·ªói x·∫£y ra khi l∆∞u model l√™n HDFS!")
    
    return success, hdfs_model_path

# Example: Load model from HDFS and make predictions
def load_and_predict_example(hdfs_model_path):
    """V√≠ d·ª• t·∫£i model t·ª´ HDFS v√† th·ª±c hi·ªán d·ª± ƒëo√°n"""
    
    # T·∫°o SparkSession
    spark = create_spark_session_with_hdfs()
    
    # T·∫£i model t·ª´ HDFS
    print(f"ƒêang t·∫£i model t·ª´ HDFS: {hdfs_model_path}")
    model = load_model_from_hdfs(hdfs_model_path)
    
    if model is None:
        print("Kh√¥ng th·ªÉ t·∫£i model t·ª´ HDFS!")
        return False
    
    # T·∫°o d·ªØ li·ªáu test
    test_data = spark.createDataFrame([
        (6.0, 7.0, 8.0),
        (7.0, 8.0, 9.0),
        (8.0, 9.0, 10.0)
    ], ["feature1", "feature2", "feature3"])
    
    # Th·ª±c hi·ªán d·ª± ƒëo√°n
    print("ƒêang th·ª±c hi·ªán d·ª± ƒëo√°n...")
    predictions = model.transform(test_data)
    
    # Hi·ªÉn th·ªã k·∫øt qu·∫£
    print("K·∫øt qu·∫£ d·ª± ƒëo√°n:")
    predictions.select("feature1", "feature2", "feature3", "prediction").show()
    
    return True

# Example: Manage HDFS directories for models
def manage_hdfs_model_directories():
    """V√≠ d·ª• qu·∫£n l√Ω th∆∞ m·ª•c models tr√™n HDFS"""
    
    from pyspark.sql import SparkSession
    
    spark = SparkSession.getActiveSession()
    if spark is None:
        spark = create_spark_session_with_hdfs()
    
    # L·∫•y Hadoop Configuration
    hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()
    
    # Import Java classes
    from py4j.java_gateway import java_import
    java_import(spark.sparkContext._gateway.jvm, "org.apache.hadoop.fs.FileSystem")
    java_import(spark.sparkContext._gateway.jvm, "org.apache.hadoop.fs.Path")
    
    FileSystem = spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.FileSystem
    Path = spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.Path
    
    fs = FileSystem.get(hadoop_conf)
    
    # T·∫°o th∆∞ m·ª•c ch√≠nh cho models
    models_dir = Path("hdfs://namenode:9000/models")
    if not fs.exists(models_dir):
        print("T·∫°o th∆∞ m·ª•c /models tr√™n HDFS...")
        fs.mkdirs(models_dir)
        print("ƒê√£ t·∫°o th∆∞ m·ª•c /models")
    else:
        print("Th∆∞ m·ª•c /models ƒë√£ t·ªìn t·∫°i tr√™n HDFS")
    
    # T·∫°o c√°c th∆∞ m·ª•c con cho c√°c lo·∫°i model kh√°c nhau
    subdirs = [
        "hdfs://namenode:9000/models/stock_prediction",
        "hdfs://namenode:9000/models/sentiment_analysis", 
        "hdfs://namenode:9000/models/news_classification",
        "hdfs://namenode:9000/models/archived"
    ]
    
    for subdir in subdirs:
        subdir_path = Path(subdir)
        if not fs.exists(subdir_path):
            print(f"T·∫°o th∆∞ m·ª•c {subdir}...")
            fs.mkdirs(subdir_path)
        else:
            print(f"Th∆∞ m·ª•c {subdir} ƒë√£ t·ªìn t·∫°i")
    
    # Li·ªát k√™ t·∫•t c·∫£ c√°c model hi·ªán c√≥
    print("\n--- Danh s√°ch models tr√™n HDFS ---")
    list_hdfs_models(fs, models_dir)

def list_hdfs_models(fs, models_dir):
    """Li·ªát k√™ t·∫•t c·∫£ models trong th∆∞ m·ª•c HDFS"""
    
    try:
        file_status_list = fs.listStatus(models_dir)
        if len(file_status_list) == 0:
            print("Kh√¥ng c√≥ model n√†o trong th∆∞ m·ª•c")
            return
        
        for file_status in file_status_list:
            path = file_status.getPath()
            name = path.getName()
            is_dir = file_status.isDirectory()
            size = file_status.getLen()
            modification_time = file_status.getModificationTime()
            
            # Chuy·ªÉn ƒë·ªïi timestamp th√†nh ƒë·ªãnh d·∫°ng ƒë·ªçc ƒë∆∞·ª£c
            import datetime
            mod_time_str = datetime.datetime.fromtimestamp(modification_time / 1000).strftime('%Y-%m-%d %H:%M:%S')
            
            if is_dir:
                print(f"üìÅ {name}/ - Th∆∞ m·ª•c - {mod_time_str}")
                # ƒê·ªá quy li·ªát k√™ n·ªôi dung th∆∞ m·ª•c con
                sub_files = fs.listStatus(path)
                for sub_file in sub_files[:3]:  # Ch·ªâ hi·ªÉn th·ªã 3 file ƒë·∫ßu
                    sub_name = sub_file.getPath().getName()
                    sub_size = sub_file.getLen()
                    print(f"  üìÑ {sub_name} ({sub_size} bytes)")
                if len(sub_files) > 3:
                    print(f"  ... v√† {len(sub_files) - 3} file kh√°c")
            else:
                print(f"üìÑ {name} - {size} bytes - {mod_time_str}")
                
    except Exception as e:
        print(f"L·ªói khi li·ªát k√™ models: {e}")

# Example: Backup and restore models
def backup_model_to_local(hdfs_model_path, local_backup_path):
    """Sao l∆∞u model t·ª´ HDFS v·ªÅ local filesystem"""
    
    from pyspark.sql import SparkSession
    import subprocess
    import os
    
    spark = SparkSession.getActiveSession()
    if spark is None:
        spark = create_spark_session_with_hdfs()
    
    try:
        # T·∫°o th∆∞ m·ª•c backup local n·∫øu ch∆∞a t·ªìn t·∫°i
        os.makedirs(os.path.dirname(local_backup_path), exist_ok=True)
        
        # S·ª≠ d·ª•ng hadoop fs command ƒë·ªÉ copy
        cmd = f"hadoop fs -copyToLocal {hdfs_model_path} {local_backup_path}"
        print(f"ƒêang th·ª±c hi·ªán backup: {cmd}")
        
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        
        if result.returncode == 0:
            print(f"TH√ÄNH C√îNG: Model ƒë√£ ƒë∆∞·ª£c backup t·ª´ {hdfs_model_path} v·ªÅ {local_backup_path}")
            return True
        else:
            print(f"L·ªñI: Kh√¥ng th·ªÉ backup model. Error: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"L·ªói x·∫£y ra khi backup model: {e}")
        return False

def restore_model_from_local(local_model_path, hdfs_restore_path):
    """Kh√¥i ph·ª•c model t·ª´ local filesystem l√™n HDFS"""
    
    from pyspark.sql import SparkSession
    import subprocess
    import os
    
    spark = SparkSession.getActiveSession()
    if spark is None:
        spark = create_spark_session_with_hdfs()
    
    try:
        # Ki·ªÉm tra file local c√≥ t·ªìn t·∫°i kh√¥ng
        if not os.path.exists(local_model_path):
            print(f"L·ªñI: File local {local_model_path} kh√¥ng t·ªìn t·∫°i!")
            return False
        
        # S·ª≠ d·ª•ng hadoop fs command ƒë·ªÉ copy
        cmd = f"hadoop fs -copyFromLocal {local_model_path} {hdfs_restore_path}"
        print(f"ƒêang th·ª±c hi·ªán restore: {cmd}")
        
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        
        if result.returncode == 0:
            print(f"TH√ÄNH C√îNG: Model ƒë√£ ƒë∆∞·ª£c restore t·ª´ {local_model_path} l√™n {hdfs_restore_path}")
            return True
        else:
            print(f"L·ªñI: Kh√¥ng th·ªÉ restore model. Error: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"L·ªói x·∫£y ra khi restore model: {e}")
        return False

# Main execution example
if __name__ == "__main__":
    print("=== HDFS Model Management Example ===\n")
    
    # B∆∞·ªõc 1: Thi·∫øt l·∫≠p th∆∞ m·ª•c HDFS
    print("1. Thi·∫øt l·∫≠p th∆∞ m·ª•c models tr√™n HDFS...")
    manage_hdfs_model_directories()
    
    # B∆∞·ªõc 2: Hu·∫•n luy·ªán v√† l∆∞u model
    print("\n2. Hu·∫•n luy·ªán v√† l∆∞u model...")
    success, model_path = train_and_save_model_example()
    
    if success:
        # B∆∞·ªõc 3: T·∫£i v√† s·ª≠ d·ª•ng model
        print("\n3. T·∫£i model v√† th·ª±c hi·ªán d·ª± ƒëo√°n...")
        load_and_predict_example(model_path)
        
        # B∆∞·ªõc 4: Backup model (optional)
        print("\n4. Backup model v·ªÅ local...")
        backup_model_to_local(model_path, "/tmp/backup/stock_model")
        
    print("\n=== Ho√†n t·∫•t ===")

# Utility functions for model versioning
def save_model_with_version(model, base_path, version=None):
    """L∆∞u model v·ªõi versioning tr√™n HDFS"""
    
    import datetime
    
    if version is None:
        # T·ª± ƒë·ªông t·∫°o version based on timestamp
        version = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    versioned_path = f"{base_path}_v{version}"
    
    print(f"L∆∞u model v·ªõi version: {version}")
    success = save_model(model, versioned_path)
    
    if success:
        # T·∫°o symlink t·ªõi latest version
        latest_path = f"{base_path}_latest"
        try:
            from pyspark.sql import SparkSession
            spark = SparkSession.getActiveSession()
            hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()
            
            from py4j.java_gateway import java_import
            java_import(spark.sparkContext._gateway.jvm, "org.apache.hadoop.fs.FileSystem")
            java_import(spark.sparkContext._gateway.jvm, "org.apache.hadoop.fs.Path")
            
            FileSystem = spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.FileSystem
            Path = spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.Path
            
            fs = FileSystem.get(hadoop_conf)
            latest_path_obj = Path(latest_path)
            
            # X√≥a latest c≈© n·∫øu c√≥
            if fs.exists(latest_path_obj):
                fs.delete(latest_path_obj, True)
            
            # T·∫°o m·ªôt file marker ƒë·ªÉ ƒë√°nh d·∫•u latest version
            version_marker_path = f"{base_path}_latest_version"
            version_marker_obj = Path(version_marker_path)
            
            if fs.exists(version_marker_obj):
                fs.delete(version_marker_obj, True)
            
            # Ghi version info v√†o file marker
            output_stream = fs.create(version_marker_obj)
            output_stream.write(version.encode('utf-8'))
            output_stream.close()
            
            print(f"Model version {version} ƒë√£ ƒë∆∞·ª£c ƒë√°nh d·∫•u l√† latest")
            
        except Exception as e:
            print(f"Kh√¥ng th·ªÉ t·∫°o latest marker: {e}")
    
    return success, versioned_path

def load_latest_model(base_path):
    """T·∫£i latest version c·ªßa model t·ª´ HDFS"""
    
    try:
        from pyspark.sql import SparkSession
        spark = SparkSession.getActiveSession()
        
        # ƒê·ªçc latest version t·ª´ marker file
        version_marker_path = f"{base_path}_latest_version"
        
        hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()
        
        from py4j.java_gateway import java_import
        java_import(spark.sparkContext._gateway.jvm, "org.apache.hadoop.fs.FileSystem")
        java_import(spark.sparkContext._gateway.jvm, "org.apache.hadoop.fs.Path")
        
        FileSystem = spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.FileSystem
        Path = spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.Path
        
        fs = FileSystem.get(hadoop_conf)
        marker_path_obj = Path(version_marker_path)
        
        if fs.exists(marker_path_obj):
            # ƒê·ªçc version t·ª´ marker file
            input_stream = fs.open(marker_path_obj)
            version = input_stream.read().decode('utf-8')
            input_stream.close()
            
            versioned_path = f"{base_path}_v{version}"
            print(f"T·∫£i latest model version: {version}")
            return load_model_from_hdfs(versioned_path)
        else:
            print("Kh√¥ng t√¨m th·∫•y latest version marker")
            return None
            
    except Exception as e:
        print(f"L·ªói khi t·∫£i latest model: {e}")
        return None