

services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - "9870:9870" # NameNode Web UI
      - "9000:9000" # HDFS IPC
    volumes:
      - namenode_data:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=stock_cluster
    env_file:
      - ./hadoop.env
    networks:
      - spark-network

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    depends_on:
      - namenode
    volumes:
      - datanode_data:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870" # Đợi NameNode UI sẵn sàng trên port 9870
    env_file:
      - ./hadoop.env
    networks:
      - spark-network

  spark-master:
    image: bitnami/spark:3.4.1
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - '8080:8080'
      - '7077:7077'
    networks:
      - spark-network

  spark-worker:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: spark-worker # Giữ tên này nếu chỉ có 1 worker
    depends_on:
      - spark-master
      - namenode # Worker cũng nên thấy NameNode
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - '8081:8081'
    volumes:
      - .:/app
    networks:
      - spark-network

  spark-app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: spark-app-submitter
    depends_on:
      - spark-master
      - elasticsearch # Thêm elasticsearch vào depends_on nếu app ghi vào đó
      - namenode    # QUAN TRỌNG: spark-app cần HDFS sẵn sàng
      - datanode    # Thêm datanode để chắc chắn hơn
    environment:
      - SPARK_MASTER_URL_FOR_PYTHON_CODE=spark://spark-master:7077
      - PYTHONUNBUFFERED=1
      - NEWS_ARTICLES_TOPIC=news_articles
      - ELASTICSEARCH_HOST=elasticsearch
      - ELASTICSEARCH_PORT=9200
      - ES_PREDICTION_INDEX=stock_predictions
      # Biến môi trường để Python code biết HDFS path (sử dụng trong src/config.py)
      - HDFS_NAMENODE_URL=hdfs://namenode:9000
      - HADOOP_USER_NAME=spark
    volumes:
      # Giữ nguyên các mount cụ thể nếu bạn muốn, hoặc đơn giản hóa thành .:/app
      - ./__init__.py:/app/__init__.py
      - ./main.py:/app/main.py
      - ./src:/app/src
      - ./models:/app/models
      - ./data:/app/data
      - spark_checkpoint:/app/checkpoint
    networks:
      - spark-network
    command: >
      sh -c "echo '--- Waiting for HDFS to be ready (simple check) ---' && \
      sleep 15 && \
      echo '--- Listing contents of /app (from spark-app) ---' && \
      ls -l /app && \
      echo '--- Attempting to submit Spark job in TRAIN mode ---' && \
      spark-submit \
      --master spark://spark-master:7077 \
      --deploy-mode client \
      --conf spark.executorEnv.PYTHONPATH=/app \
      --conf spark.hadoop.fs.defaultFS=hdfs://namenode:9000 \
      --packages org.elasticsearch:elasticsearch-spark-30_2.12:8.11.0 \
      /app/main.py train" 

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: elasticsearch
    environment:
      - xpack.security.enabled=false
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
    ports:
      - '9200:9200'
      - '9300:9300'
    networks:
      - spark-network
    volumes:
      - esdata:/usr/share/elasticsearch/data

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: kibana
    depends_on:
      - elasticsearch
    ports:
      - '5601:5601'
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    networks:
      - spark-network

volumes:
  namenode_data: # THÊM ĐỊNH NGHĨA
    driver: local
  datanode_data: # THÊM ĐỊNH NGHĨA
    driver: local
  esdata:
    driver: local
  spark_checkpoint:
    driver: local

networks:
  spark-network:
    driver: bridge