services:
  spark-master:
    image: bitnami/spark:3.4.1
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - '8080:8080' # Spark Master Web UI
      - '7077:7077' # Spark Master internal communication
    networks:
      - spark-network

  spark-worker: # Changed from spark-worker to spark-worker-1 to match logs
    build: # ADDED: Worker will now build from Dockerfile
      context: .
      dockerfile: Dockerfile # Assumes the same Dockerfile installs requirements
    container_name: spark-worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      # Ensure PYTHONPATH is also set for worker if it runs Python processes directly (though Spark usually handles this for executors)
      # - PYTHONPATH=/app 
    ports:
      - '8081:8081' # Spark Worker Web UI
    volumes: 
      # This mount makes your project files available at /app inside the worker.
      # The Dockerfile build process will use files from the context, but this ensures runtime access if needed.
      - .:/app 
    networks:
      - spark-network

  spark-app:
    build:
      context: .
      dockerfile: Dockerfile 
    container_name: spark-app-submitter
    depends_on:
      - spark-master
      - kafka 
    environment:
      - SPARK_MASTER_URL_FOR_PYTHON_CODE=spark://spark-master:7077
      - PYTHONUNBUFFERED=1
      - KAFKA_BROKER=kafka:9092
      - NEWS_ARTICLES_TOPIC=news_articles
      - ELASTICSEARCH_HOST=elasticsearch
      - ELASTICSEARCH_PORT=9200
      - ES_PREDICTION_INDEX=stock_predictions
    volumes:
      # These specific mounts ensure the driver container (spark-app-submitter)
      # has the latest code from your host, which is good for development.
      # The build context already copies these, but volumes ensure live updates if you modify code without rebuilding.
      - ./__init__.py:/app/__init__.py 
      - ./main.py:/app/main.py       
      - ./src:/app/src 
      - ./models:/app/models
      - ./data:/app/data 
      - spark_checkpoint:/app/checkpoint
    networks:
      - spark-network
    command: >
      sh -c "echo '--- Listing contents of /app (from spark-app) ---' && \
      ls -l /app && \
      echo '--- Listing contents of /app/src (from spark-app, mounted from host ./src) ---' && \
      ls -l /app/src && \
      echo '--- Attempting to submit Spark job in TRAIN mode with executor PYTHONPATH ---' && \
      spark-submit \
      --master spark://spark-master:7077 \
      --deploy-mode client \
      --conf spark.executorEnv.PYTHONPATH=/app \
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,org.elasticsearch:elasticsearch-spark-30_2.12:8.11.0 \
      /app/main.py train"

  zookeeper:
    image: bitnami/zookeeper:3.8.3
    container_name: zookeeper
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    ports:
      - '2181:2181'
    networks:
      - spark-network

  kafka:
    image: bitnami/kafka:3.5.1
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - '9092:9092' 
      - '9093:9093' 
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_LISTENERS=PLAINTEXT://:9092,EXTERNAL://:9093
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,EXTERNAL://localhost:9093 
      - KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 
    networks:
      - spark-network

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: elasticsearch
    environment:
      - xpack.security.enabled=false
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
    ports:
      - '9200:9200'
      - '9300:9300'
    networks:
      - spark-network
    volumes:
      - esdata:/usr/share/elasticsearch/data 

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: kibana
    depends_on:
      - elasticsearch
    ports:
      - '5601:5601'
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    networks:
      - spark-network

volumes:
  esdata:
    driver: local
  spark_checkpoint: 
    driver: local

networks:
  spark-network:
    driver: bridge
